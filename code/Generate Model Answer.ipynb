{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_GPT_answer(text, question):\n",
    "\n",
    "  prompt_0 = \"\"\"\n",
    "I will provide you with a passage of text and a question . Based on the\n",
    "type of question , please follow the steps below to provide your\n",
    "answer . Determine the type of question according to the beginning of\n",
    "the question , and answer the following questions according to the\n",
    "different types :\n",
    "1. For quantitative questions (\" what is \") , you should provide a\n",
    "corresponding number based on the information in the text . If the\n",
    "text uses expressions such as \"not less \" or \" more than \" to represent\n",
    "quantities , the original representation is used . If the text does\n",
    "not mention any relevant data , respond with \"No Answer \".\n",
    "2. For qualitative questions (\" Is there any description \") , when\n",
    "answering \"Is there any description \" questions , ensure you carefully\n",
    "verify the information . If the text clearly lacks the required\n",
    "information , answer \"no \". Otherwise , answer \"yes \".\n",
    "\"\"\"\n",
    "  text_1 = \"Passage: \\n\" + str(text)\n",
    "  question_1 = \"\\n Question: \\n\" + question\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": prompt_0},\n",
    "      {\"role\": \"user\", \"content\": text_1},\n",
    "      {\"role\": \"user\", \"content\": question_1}\n",
    "    ],\n",
    "    temperature=0\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoldeAnswer(text, question):\n",
    "\n",
    "  alpaca_prompt = \"\"\"\n",
    "I will provide you with a passage of text and a question . Based on the\n",
    "type of question , please follow the steps below to provide your\n",
    "answer . Determine the type of question according to the beginning of\n",
    "the question , and answer the following questions according to the\n",
    "different types :\n",
    "1. For quantitative questions (\" what is \") , you should provide a\n",
    "corresponding number based on the information in the text . If the\n",
    "text uses expressions such as \"not less \" or \" more than \" to represent\n",
    "quantities , the original representation is used . If the text does\n",
    "not mention any relevant data , respond with \"No Answer \".\n",
    "2. For qualitative questions (\" Is there any description \") , when\n",
    "answering \"Is there any description \" questions , ensure you carefully\n",
    "verify the information . If the text clearly lacks the required\n",
    "information , answer \"no \". Otherwise , answer \"yes \".\n",
    "### Text:\n",
    "{}\n",
    "\n",
    "### Quetsion: :\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\"\"\"\n",
    "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "  inputs = tokenizer(\n",
    "  [\n",
    "      alpaca_prompt.format(\n",
    "          text, # instruction\n",
    "          question, # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "  ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "  outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "\n",
    "  return_str = tokenizer.batch_decode(outputs)\n",
    "\n",
    "  text = return_str[0]\n",
    "  start_index = text.find(\"### Answer:\")\n",
    "  if start_index != -1:\n",
    "    start_index = start_index  + len(\"### Answer:\")\n",
    "    result = text[start_index:]\n",
    "  else:\n",
    "    result = \"No Answer\"\n",
    "\n",
    "  finish_index = result.find(\"### Quetsion:\")\n",
    "  if finish_index != -1:\n",
    "    #start_index = start_index  + len(\"### Answer:\")\n",
    "    result = result[:finish_index]\n",
    "  return result.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "modle_Answer = []\n",
    "original_Information = []\n",
    "question = []\n",
    "gold_Answer = []\n",
    "question_Type = []\n",
    "\n",
    "for index, row in df_QA_data.iterrows():\n",
    "    print(index)\n",
    "    original_Information.append(row['original_Information'])\n",
    "    question.append(row['question'])\n",
    "    gold_Answer.append(row['gold_Answer'])\n",
    "    #change this function\n",
    "    model_Answer_get = getMoldeAnswer(row['original_Information'],row['question'])\n",
    "\n",
    "    modle_Answer.append(model_Answer_get)\n",
    "finish_time = time.time()\n",
    "print(finish_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_sample = {'original_Information':original_Information,\n",
    "         'question':question,\n",
    "            'gold_Answer':gold_Answer,\n",
    "             'modle_Answer':modle_Answer}\n",
    "\n",
    "df_qa_sample_ss = pd.DataFrame(qa_sample)\n",
    "fp = 'file_name.xlsx'\n",
    "df_qa_sample_ss.to_excel(fp, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
